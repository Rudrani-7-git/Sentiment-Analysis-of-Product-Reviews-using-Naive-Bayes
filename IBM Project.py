# -*- coding: utf-8 -*-
"""Untitled1 ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13LIHz1-b9xZlpoWGgJPQGBw7pczvYp_O
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import nltk
from nltk.stem.porter import PorterStemmer
nltk.download('stopwords')
from nltk.corpus import stopwords
STOPWORDS =set(stopwords.words('english'))
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.feature_extraction.text import CountVectorizer
from wordcloud import WordCloud
import pickle
import re

!pip install matplotlib-venn

# https://pypi.python.org/pypi/pydot
!apt-get -qq install -y graphviz && pip install pydot
import pydot

!pip install cartopy

import cartopy

# Commented out IPython magic to ensure Python compatibility.
# %pip install wordcloud

"""***Data Analysiss***

"""

#Load the data

data = pd.read_csv(r"/content/drive/MyDrive/flipkart_alexa.tsv", sep='\t' , quoting = 3)

print(f"Dataset shape : {data.shape}")

data.head()

#Column names

print(f"Feature names : {data.columns.values}")

#Check for null values

data.isnull().sum()

#Getting the record where 'verified_reviews' is null

data[data['verified_reviews'].isna() == True]

#We will drop the null record

data.dropna(inplace=True)
print(f"Dataset shape after dropping null values : {data.shape}")

#Creating a new column 'length' that will contain the length of the string in 'verified_reviews' column

data['length'] = data['verified_reviews'].apply(len)

data.head()

"""**The 'length' column is new generated column - stores the length of 'verified_reviews' for that record. Let's check for some sample records**"""

#Randomly checking for 10th record

print(f"'verified_reviews' column value: {data.iloc[10]['verified_reviews']}") #Original value
print(f"Length of review : {len(data.iloc[10]['verified_reviews'])}") #Length of review using len()
print(f"'length' column value : {data.iloc[10]['length']}") #Value of the column 'length'

"""We can see that the length of review is the same as the value in the length column for that record

Datatypes of the features
"""

data.dtypes

"""*  rating, feedback and length are integer values
* date, variation and verified_reviews are string values

# Analyzing 'rating' column

This column refers to the rating of the variation given by the user
"""

len(data)

#Distinct values of 'rating' and its count

print(f"Rating value count: \n{data['rating'].value_counts()}")

"""*Let's plot the above values in a bar graph*"""

#Bar plot to visualize the total counts of each rating

data['rating'].value_counts().plot.bar(color = 'red')
plt.title('Rating distribution count')
plt.xlabel('Ratings')
plt.ylabel('Count')
plt.show()

#Finding the percentage distribution of each rating - we'll divide the number of records for each rating by total number of records

print(f"Rating value count - percentage distribution: \n{round(data['rating'].value_counts()/data.shape[0]*100,2)}")

"""*Let's plot the above values in a pie chart*"""

fig = plt.figure(figsize=(7,7))

colors = ('maroon', 'tan', 'turquoise','gold','coral')

wp = {'linewidth':1, "edgecolor":'black'}

tags = data['rating'].value_counts()/data.shape[0]

explode=(0.1,0.1,0.1,0.1,0.1)

tags.plot(kind='pie', autopct="%1.1f%%", shadow=True, colors=colors, startangle=90, wedgeprops=wp, explode=explode, label='Percentage wise distrubution of rating')

from io import  BytesIO

graph = BytesIO()

fig.savefig(graph, format="png")

"""### Analyzing 'feedback' column"""

#This coloumn refers to the feedback of the verified review
#Distinct values of 'feedback' and its count

print(f"Feedback value count: \n{data['feedback'].value_counts()}")

"""There are 2 distinct values of 'feedback' present - 0 and 1. Let's see what kind of review each value corresponds to.

feedback value = 0
"""

#Extracting the 'verified_reviews' value for one record with feedback = 0

review_0 = data[data['feedback'] == 0].iloc[1]['verified_reviews']
print(review_0)

#Extracting the 'verified_reviews' value for one record with feedback = 1

review_1 = data[data['feedback'] == 1].iloc[1]['verified_reviews']
print(review_1)

"""*From the above 2 examples we can see that feedback 0 is negative review and 1 is positive review*"""

#Bar graph to visualize the total counts of each feedback

data['feedback'].value_counts().plot.bar(color = 'brown')
plt.title('Feedback distribution count')
plt.xlabel('Feedback')
plt.ylabel('Count')
plt.show()

#Finding the percentage distribution of each feedback - we'll divide the number of records for each feedback by total number of records

print(f"Feedback value count - percentage distribution: \n{round(data['feedback'].value_counts()/data.shape[0]*100,2)}")

"""Feedback distribution

*  91.87% reviews are positive
*  8.13% reviews are negative
"""

fig = plt.figure(figsize=(7,7))

colors = ('brown', 'thistle')

wp = {'linewidth':1, "edgecolor":'black'}

tags = data['feedback'].value_counts()/data.shape[0]

explode=(0.1,0.1)

tags.plot(kind='pie', autopct="%1.1f%%", shadow=True, colors=colors, startangle=90, wedgeprops=wp, explode=explode, label='Percentage wise distrubution of feedback')

"""*Let's see the 'rating' values for different values of 'feedback'*"""

#Feedback = 0
data[data['feedback'] == 0]['rating'].value_counts()

#Feedback = 1
data[data['feedback'] == 1]['rating'].value_counts()

"""### Analyzing 'variation' column

*This column refers to the variation or type of Amazon Alexa product. Example - Black Dot, Charcoal Fabric etc.*
"""

#Distinct values of 'variation' and its count

print(f"Variation value count: \n{data['variation'].value_counts()}")

#Bar graph to visualize the total counts of each variation

data['variation'].value_counts().plot.bar(color = 'orange')
plt.title('Variation distribution count')
plt.xlabel('Variation')
plt.ylabel('Count')
plt.show()

#Finding the percentage distribution of each variation - we'll divide the number of records for each variation by total number of records

print(f"Variation value count - percentage distribution: \n{round(data['variation'].value_counts()/data.shape[0]*100,2)}")

data.groupby('variation')['rating'].mean()

data.groupby('variation')['rating'].mean().sort_values().plot.bar(color = 'skyblue', figsize=(11, 6))
plt.title("Mean rating according to variation")
plt.xlabel('Variation')
plt.ylabel('Mean rating')
plt.show()

"""### Analyzing 'verified_reviews' column
This column contains the textual review given by the user for a variation for the product.
"""

data['length'].describe()

"""Name: length, dtype: float64
Length analysis for full dataset
"""

sns.histplot(data['length'],color='navy').set(title='Distribution of length of review ')

"""Length analysis when feedback is 0 (negative)"""

sns.histplot(data[data['feedback']==0]['length'],color='gold').set(title='Distribution of length of review if feedback = 0')

"""Length analysis when feedback is 1 (positive)"""

sns.histplot(data[data['feedback']==1]['length'],color='green').set(title='Distribution of length of review if feedback = 1')

data.groupby('length')['rating'].mean().plot.hist(color = 'navy', figsize=(7, 6), bins = 20)
plt.title(" Review length wise mean ratings")
plt.xlabel('ratings')
plt.ylabel('length')
plt.show()

cv = CountVectorizer(stop_words='english')
words = cv.fit_transform(data.verified_reviews)

# Combine all reviews
reviews = " ".join([review for review in data['verified_reviews']])

# Initialize wordcloud object
wc = WordCloud(background_color='white', max_words=50)

# Generate and plot wordcloud
plt.figure(figsize=(10,10))
plt.imshow(wc.generate(reviews))
plt.title('Wordcloud for all reviews', fontsize=10)
plt.axis('off')
plt.show()

"""Lets find the unique words in each feedback category"""

# Combine all reviews for each feedback category and splitting them into individual words
neg_reviews = " ".join([review for review in data[data['feedback'] == 0]['verified_reviews']])
neg_reviews = neg_reviews.lower().split()

pos_reviews = " ".join([review for review in data[data['feedback'] == 1]['verified_reviews']])
pos_reviews = pos_reviews.lower().split()

#Finding words from reviews which are present in that feedback category only
unique_negative = [x for x in neg_reviews if x not in pos_reviews]
unique_negative = " ".join(unique_negative)

unique_positive = [x for x in pos_reviews if x not in neg_reviews]
unique_positive = " ".join(unique_positive)

wc = WordCloud(background_color='white', max_words=50)

# Generate and plot wordcloud
plt.figure(figsize=(10,10))
plt.imshow(wc.generate(unique_negative))
plt.title('Wordcloud for negative reviews', fontsize=10)
plt.axis('off')
plt.show()

"""Negative words can be seen in the above word cloud - garbage, pointless, poor, horrible, repair etc"""

wc = WordCloud(background_color='white', max_words=50)

# Generate and plot wordcloud
plt.figure(figsize=(10,10))
plt.imshow(wc.generate(unique_positive))
plt.title('Wordcloud for positive reviews', fontsize=10)
plt.axis('off')
plt.show()

"""Positive words can be seen in the above word cloud - good, enjoying, amazing, best, great etc

# ***THE END !!!!!***
"""